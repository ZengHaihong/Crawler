{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动获取ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import HTTPError,Timeout,ConnectionError,ChunkedEncodingError\n",
    "import time\n",
    "import re\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Ip_pool():\n",
    "    '''\n",
    "    这是一个获取可用的ip池的类\n",
    "    \n",
    "    功能\n",
    "    -----\n",
    "    获取西刺代理高匿代理ip\n",
    "    \n",
    "    属性\n",
    "    -----\n",
    "    ip_url_ls:字符串，代理ip的爬取页面初始页面\n",
    "    \n",
    "    方法\n",
    "    -----\n",
    "    1.check_chinese(self,check_str): 检查ip地址是否含中文字符\n",
    "    2.visit_url(self,ip_url,prox_ip=None):访问西刺代理ip，获得BeautifulSoup\n",
    "    3.get_iplist(self,soup)：从BeautifulSoup对象获取代理ip信息\n",
    "    4.get_ip_form(self,result=None)：从ip信息提取ip格式\n",
    "    5.check_ip(self,test_url,ip_list=None,timeout=10):拿一个网站去检查ip是否可用\n",
    "    6.get_ip_pool(self,test_url=\"https://music.163.com/\",page_num = 4,timeout = 3)：获取ip池\n",
    "    '''\n",
    "    def __init__(self,ip_url_ls= 'http://www.xicidaili.com/nn/'):\n",
    "        print(\"-----从西刺代理获取代理IP----------\")\n",
    "        self.ip_url_ls = ip_url_ls\n",
    "    \n",
    "    def check_chinese(self,check_str):\n",
    "        '''\n",
    "        功能\n",
    "        ----\n",
    "        检查ip号是否还有中文字符，如果有中文字符则去掉\n",
    "        \n",
    "        参数\n",
    "        ----\n",
    "        check_str:字符串，表示代理ip号\n",
    "        \n",
    "        返回\n",
    "        ---\n",
    "        返回True或者False\n",
    "        '''\n",
    "        zhPattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "        match = zhPattern.search(check_str)\n",
    "        if match:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def visit_url(self,ip_url,prox_ip=None):\n",
    "        '''\n",
    "        功能\n",
    "        ------\n",
    "        访问西刺代理网站，获取代理ip\n",
    "        \n",
    "        参数\n",
    "        ------\n",
    "        ip_url:字符串，爬取西刺代理url的初始入口\n",
    "        prox_ip:列表，如[\"192.1681.1.1\":“800”]\n",
    "        \n",
    "        返回值\n",
    "        ------\n",
    "        soup:BeautifulSoup对象，用来提取代理ip\n",
    "        '''\n",
    "        global headers\n",
    "        headers={ \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "         \"Accept-Language\": \"zh-CN,zh;q=0.8\",\n",
    "         \"Connection\": \"keep-alive\",\n",
    "         \"Content-Type\":\"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "         'User-Agent':\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.103 Safari/537.36\"}\n",
    "        is_continue = True  #设置循环次数\n",
    "        while is_continue:\n",
    "            try:\n",
    "                if prox_ip is None:\n",
    "                    res = requests.get(ip_url,headers = headers)  #访问url，不设代理ip访问\n",
    "                else:\n",
    "                    http = prox_ip[0]   \n",
    "                    ip = http+\"://\"+prox_ip[1]\n",
    "                    proxies={http:ip}\n",
    "                    res = requests.get(self.ip_url,headers = headers,proxies = proxies) #设代理ip访问\n",
    "                html =  res.content.decode('utf-8')\n",
    "                soup  =  BeautifulSoup(html,'lxml')\n",
    "                is_continue = False \n",
    "            \n",
    "            except HTTPError as e:\n",
    "                print(\"---断网---\")\n",
    "                time.sleep(2)\n",
    "\n",
    "            except Timeout as e:\n",
    "                print(\"---请求超时---\")\n",
    "                time.sleep(2)\n",
    "\n",
    "            except ConnectionError as e:\n",
    "                print(\"---访问被拒---\")\n",
    "                is_continue = True\n",
    "                time.sleep(2)\n",
    "\n",
    "        return soup\n",
    "\n",
    "\n",
    "    #*******获取ip信息**********\n",
    "    def get_iplist(self,soup):\n",
    "\n",
    "        '''\n",
    "        功能：对解析后的网页进行清洗，获得目标变量\n",
    "\n",
    "        参数\n",
    "        ----\n",
    "        soup：BeautifulSoup对象，网页元素经过解析后的对象，包含了ip、端口号以及其他信息。\n",
    "\n",
    "        返回值\n",
    "        -----\n",
    "        result:数组，装载了清洗得到的目标内容，如何ip、端口号以及其他信息\n",
    "        '''\n",
    "        result = []\n",
    "        for i in soup.select(\"#ip_list .odd\"):  #遍历每一组信息\n",
    "            string = \"\"                         #初始化空字符\n",
    "            for j in i.select(\"td\"):           #信息存在td标签的text中\n",
    "                if j.text.strip()!='':         #过滤掉没有text的td标签\n",
    "                    info = j.text.strip()      #去除两边的空字符\n",
    "                    string = string + info + \"|\"\n",
    "\n",
    "            for m in i.select(\".bar\"):      #反应速度和连接时长都在此处\n",
    "                time = m[\"title\"]           #反应时间和连接时间\n",
    "                string = string+time+\"|\"    #\n",
    "            result.append(string)           #保存每一组的string\n",
    "        return result\n",
    "\n",
    "    def get_ip_form(self,result=None):\n",
    "        '''\n",
    "        功能：对获取的ip信息进行整合成符合requests的形式\n",
    "\n",
    "        参数\n",
    "        ----\n",
    "        result：数组，装载了清洗得到的目标内容，如何ip、端口号以及其他信息\n",
    "\n",
    "        返回值\n",
    "        -----\n",
    "        ip_list:数组，获取新的ip组\n",
    "\n",
    "        '''\n",
    "        ip_list = []                  #用来装整合的ip\n",
    "        for i in result:              #遍历每一条ip信息\n",
    "            if 'sock' not in i:       #删选socket类的代理ip\n",
    "                a = i.split(\"|\")      #分割\n",
    "                type1 = a[4].lower() #将代理ip的类型全部变成小写\n",
    "                ip = a[0]            #ip号\n",
    "                port=a[1]            #代理ip端口\n",
    "                ip_list.append({type1:type1+\"://\"+ip+\":\"+port}) #添加整合好的ip记录\n",
    "        return ip_list\n",
    "\n",
    "    def check_ip(self,test_url,ip_list=None,timeout=10):\n",
    "        '''\n",
    "        功能：对每一个ip进行测试，返回有效的，提除无效的\n",
    "\n",
    "        参数\n",
    "        ----\n",
    "        ip_list：数组，装载代理ip的类型，代理ip和端口\n",
    "        url：字符串，用来检测ip是否可用的url\n",
    "        timeout:整型，访问时间上限，如果超过该时间限制仍访问不成功，即报错。\n",
    "\n",
    "        返回值\n",
    "        -----\n",
    "        new_ip_list:数组，获取新的ip组\n",
    "\n",
    "        '''\n",
    "        new_ip_list = [] \n",
    "        for i in ip_list:\n",
    "            try:\n",
    "                if '天' not in list(i.values())[0] and not self.check_chinese(list(i.values())[0]):\n",
    "                    res = requests.get(test_url,proxies=i,timeout=timeout,headers=headers)\n",
    "                if res.status_code == 200:\n",
    "                    new_ip_list.append(i)\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                return new_ip_list\n",
    "        return new_ip_list\n",
    "    \n",
    "\n",
    "    def get_ip_pool(self,test_url=\"https://music.163.com/\",page_num = 4,timeout = 3):\n",
    "        '''\n",
    "        功能：获取代理ip网站的ip列表，同时对指定的网站进行测试\n",
    "        \n",
    "        参数\n",
    "        ------\n",
    "        test_url:字符串，表示的是测试ip能否访问的网站，默认为网易云音乐的官方网站。\n",
    "        \n",
    "        timeout:数值型，最大采取时间。\n",
    "        \n",
    "        page_num:int型，爬取西刺网最多的页面数。\n",
    "        \n",
    "        返回值\n",
    "        -------\n",
    "        new_ip_pool:列表，返回经过检查后的ip列表\n",
    "        '''\n",
    "        ip_pool = []\n",
    "        #**********获取西刺代理的ip，请检验*******************\n",
    "        for num in range(1,page_num+1):\n",
    "            ip_url = self.ip_url_ls + str(num)\n",
    "            print(\"正在爬取{0}\".format(ip_url))\n",
    "            soup = self.visit_url(ip_url)   ##访问西刺网\n",
    "            result = self.get_iplist(soup)  #获取第一页的ip结果\n",
    "            ip_list = self.get_ip_form(result=result) #获取ip形式\n",
    "            print(\"正在测试ip能否访问{0}\".format(test_url))\n",
    "            new_ip_list = self.check_ip(test_url,ip_list,timeout) #检查ip\n",
    "            ip_pool.extend(new_ip_list)\n",
    "    \n",
    "        #*******对ip池进行去重*******************\n",
    "        new_ip_pool = []\n",
    "        for i in ip_pool:\n",
    "            if i not in new_ip_pool:\n",
    "                new_ip_pool.append(i)\n",
    "        print(\"一共获取了{0}个ip\".format(len(new_ip_pool)))\n",
    "        return new_ip_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----从西刺代理获取代理IP----------\n",
      "正在爬取http://www.xicidaili.com/nn/1\n",
      "正在测试ip能否访问https://music.163.com/\n",
      "一共获取了20个ip\n"
     ]
    }
   ],
   "source": [
    "new_ip_pool = Ip_pool().get_ip_pool(page_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 继承"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Crawl_common(object):\n",
    "    '''\n",
    "    这是用来爬取大众点评的共有包，包含了以下的函数。\n",
    "    \n",
    "    函数\n",
    "    -------\n",
    "    \n",
    "    1. requests_visit_url(self,url,is_use_ip =False,timeout=3)\n",
    "        函数功能：访问特定的url，带有ip池的功能\n",
    "        当is_use_ip等于False时，即不用代理ip池去访问，如果is_use_ip等于True时，即使用代理池去访问\n",
    "          \n",
    "        \n",
    "    '''\n",
    "    def __init__(self,test_url = 'https://music.163.com/',page_num=3,timeout=3):\n",
    "        '''\n",
    "        功能：生成ip池\n",
    "        \n",
    "        参数\n",
    "        ----\n",
    "        test_url:字符串，检测代理ip池是否正常工作的网址\n",
    "        page_num:整数型，获取西刺代理的\n",
    "        '''\n",
    "        self.test_url = test_url\n",
    "        self.page_num = page_num\n",
    "        self.timeout = timeout\n",
    "        global ip_pool\n",
    "        global bad_ip\n",
    "        \n",
    "        #获取代理ip池\n",
    "        ip_pool= Ip_pool().get_ip_pool(self.test_url,page_num=self.page_num,timeout=self.timeout)\n",
    "        print(\"-----------ip池已经获取完毕---------------------\")\n",
    "    \n",
    "    #*************带ip池访问url**********************************\n",
    "    def requests_visit_url(self,url,is_use_ip =True,timeout=5):\n",
    "        '''\n",
    "        功能：自带ip池功能来放访问特定的url\n",
    "        \n",
    "        参数\n",
    "        ----\n",
    "        url：sring，表示特定的ul\n",
    "        is_use_ip :bool，True表示采用ip池访问\n",
    "        timeout:int,表示一次访问url的最大超时限制\n",
    "        \n",
    "        结果\n",
    "        ----\n",
    "        soup:BeautifulSoup对象，可以用来解析网页\n",
    "        '''\n",
    "        global ip_pool\n",
    "        global bad_ip\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/61.0.3163.100 Chrome/61.0.3163.100 Safari/537.36\"}\n",
    "        bad_ip = []  #用来装坏的ip\n",
    "        \n",
    "        #*********如果不设置ip的话**********************************\n",
    "        if is_use_ip == False:\n",
    "            res = requests.get(url,headers = headers,timeout=timeout)  #访问url，不设代理ip访问\n",
    "            html =  res.content.decode('utf-8')\n",
    "            soup  =  BeautifulSoup(html,'lxml')\n",
    "            if soup.title.text == '403 Forbidden' or \"有道\" in soup.title.text:\n",
    "                print(\"ip被禁止了！！！！！！！！！！！！！！！！\")\n",
    "                raise ConnectionError\n",
    "           \n",
    "        #************如果设置ip的话，则执行这一段*********************\n",
    "        else:\n",
    "            is_continue = True   #设置循环标志\n",
    "            while is_continue:\n",
    "            #整理成ip地址的格式\n",
    "                try:\n",
    "                    #如果无效的ip等于整个ip池的ip个数时，重新再爬一次\n",
    "                    if len(ip_pool)==len(bad_ip):\n",
    "                        ip_url = 'http://www.xicidaili.com/nn/'\n",
    "                        ip_pool= Ip_pool().get_ip_pool(self.test_url,timeout=2,page_num=3)\n",
    "                        proxies = random.choice(ip_pool)\n",
    "                        bad_ip = []\n",
    "                    #******随机生成一个ip*******\n",
    "                    proxies = random.choice(ip_pool)\n",
    "                    \n",
    "                    #*******用代理ip访问********\n",
    "                    res = requests.get(url,headers = headers,proxies = proxies,timeout=timeout) #设代理ip访问\n",
    "                    html =  res.content.decode('utf-8')\n",
    "                    soup  =  BeautifulSoup(html,'lxml')\n",
    "                \n",
    "                    #********如果返回网页被禁止的情况，触发异常,否则返回正常**********\n",
    "                    if  soup.title.text == '403 Forbidden' or \"有道\" in soup.title.text:\n",
    "                        print(\"ip被禁止了!!!!!!!!!!!!!!!!!!!!\")\n",
    "                        raise ConnectionError\n",
    "                    else:\n",
    "                        is_continue = False\n",
    "\n",
    "                except HTTPError as e:\n",
    "                    bad_ip.append(proxies)\n",
    "                    print(\"断网\")\n",
    "                    proxies = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "\n",
    "\n",
    "                except Timeout as e:\n",
    "                    bad_ip.append(proxies)\n",
    "                    print(\"超时\")\n",
    "                    proxies = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "\n",
    "\n",
    "                except ConnectionError as e:\n",
    "                    bad_ip.append(proxies)\n",
    "                    print(\"访问被拒\")\n",
    "                    proxies = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except AttributeError as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"----------\")\n",
    "                    proxies = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except ChunkedEncodingError as e:\n",
    "                    bad_ip.append(proxies)\n",
    "                    print(\"ChunkedEncodingError\")\n",
    "                    proxies = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
